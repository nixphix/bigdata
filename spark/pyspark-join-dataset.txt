# join two dataset in pyspark
# check if datasets are available (created in previous excercise)

hdfs dfs -ls sqoop_import/orders
# columns metadata 
# (order_id int), (order_date string), (order_customer_id int), (order_status string)
hdfs dfs -tail sqoop_import/orders/part-m-00000

hdfs dfs -ls sqoop_import/order_items
# columns metadata 
# (order_item_id int), (order_item_order_id int), (order_item_product_id int), (order_item_quantity tinyint), (order_item_subtotal double), (order_item_product_price double)
hdfs dfs -tail sqoop_import/order_items/part-m-00000

# find daily trunover
# start pyspark shell
pyspark -master yarn

# import orders table and verify data 
ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders/")
for i in ordersRDD.take(5):
    print i

# take order id and order date, parser the order id into int
ordersParsedRDD = ordersRDD.map(lambda x:(int(x.split(",")[0]),x.split(",")[1]))
for i in ordersParsedRDD.take(5): 
    print i

# import order_items table and verify data 
orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items/")
for i in orderItemsRDD.take(5):
    print i

# order id is the second column in order items table, parse it into int for joining datasets
orderItemsParsedRDD = orderItemsRDD.map(lambda x:(int(x.split(",")[1]), float(x.split(",")[4])))
for i in orderItemsParsedRDD.take(5):
    print i

# join both dataset
ordersJoinOrderItemsRDD = orderItemsParsedRDD.join(ordersParsedRDD)
for i in ordersJoinOrderItemsRDD.take(5):
    print i
    
    
    
    
